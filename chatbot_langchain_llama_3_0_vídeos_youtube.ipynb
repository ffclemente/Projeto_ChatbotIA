{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Instalando pacotes\n",
        "!pip install langchain==0.3.0\n",
        "!pip install langchain-groq==0.2.0\n",
        "!pip install langchain-community==0.3.0\n",
        "!pip install youtube_transcript_api==0.6.2\n",
        "!pip install pypdf==5.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c_sA1UVesuoZ",
        "outputId": "73f2bbae-1b65-4fcc-8225-778238eae8c2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.0\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.3.0) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.3.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.0) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.3.0) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.0) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.3.0) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.3.0) (1.2.2)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: tenacity, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "Successfully installed langchain-0.3.0 tenacity-8.5.0\n",
            "Collecting langchain-groq==0.2.0\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq==0.2.0)\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3 in /usr/local/lib/python3.10/dist-packages (from langchain-groq==0.2.0) (0.3.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.2.0) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (0.1.137)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (3.10.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq==0.2.0) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3->langchain-groq==0.2.0) (2.2.3)\n",
            "Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.11.0 langchain-groq-0.2.0\n",
            "Collecting langchain-community==0.3.0\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (0.3.13)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.0)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community==0.3.0) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community==0.3.0) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.0) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-community==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.0->langchain-community==0.3.0) (2.23.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community==0.3.0) (1.2.2)\n",
            "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.3.0 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "Collecting youtube_transcript_api==0.6.2\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api==0.6.2) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api==0.6.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api==0.6.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api==0.6.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api==0.6.2) (2024.8.30)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.2\n",
            "Collecting pypdf==5.0.0\n",
            "  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf==5.0.0) (4.12.2)\n",
            "Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar pacote Langchain para conexão com o LLAMA 3.1 através da API do site da GROQ\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Conectando API da GROQ\n",
        "# Site: https://console.groq.com/keys\n",
        "\n",
        "api_key = 'NECESSÁRIO CRIAR UMA API_KEY NO SITE DA GROQ'\n",
        "os.environ['GROQ_API_KEY'] = api_key\n",
        "chat = ChatGroq(model='llama-3.1-70b-versatile')"
      ],
      "metadata": {
        "id": "jd-QvJ4H6JIG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "\n",
        "# Caregar o link do vídeo do Youtube que deseja analisar\n",
        "# Vídeo: Aula 1 - Aplicações de inteligência artificial e machine learning em saúde (Canal USP)\n",
        "url = 'https://www.youtube.com/watch?v=y8em7JhKwhU&list=PLAudUnJeNg4tvUFZ8tXQDoAkFAASQzOHm&index=2&ab_channel=CanalUSP'\n",
        "\n",
        "# Carregar os documentos com o idioma em português (pt)\n",
        "loader = YoutubeLoader.from_youtube_url(url, language=\"pt\")\n",
        "lista_documentos = loader.load()\n",
        "lista_documentos"
      ],
      "metadata": {
        "id": "0rEajOpVWPFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5508c152-dbe7-4f90-f713-70025d86f370"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'y8em7JhKwhU'}, page_content='meu nome é alexandre eu sou professor da faculdade de saúde pública da usp a na área de estatísticas de saúde né essência de dados eu sou originalmente economista e fascinado por pela questão da análise de dados principalmente na área da saúde que é uma área tão cheia de imperfeições de mercados e novas novas interesses científicos a serem abordados com a análise de dados bem vindos ao curso de inteligência artificial com foco na área da saúde a gente vai falar sobre bastante o comércio online as possibilidades de machine as aplicações técnicas de marchi lane a gente vai começar desde o início neder e como rodar modelos de mach lane né principalmente como inserir variáveis o que fazer com essas variáveis quais variáveis importantes a gente vai falar sobre todas as técnicas iniciais de de pré processamento de dados daí vamos falar sobre os modelos de um machinho lane aí no fim vamos terminar atestando a qualidade desses modelos então você o curso que vai do início ao fim de mach lane então vai ser uma parte essa parte vai ser bastante técnica com a gente vai também vai ter parte mais gerais né principalmente no início pra explicar às pessoas que não conhece o que é essencial é explicar um pouco o que é possível dentro da área e desmistifique desmistificar um pouco essa história de inteligência artificial que parece uma coisa muito distante para muitas pessoas mas que já é uma realidade hoje a eficiência e é uma realidade hoje cada vez mais nas indústrias e nas empresas tá então bem vindo ao curso espero que todo mundo tenha bastante proveito com esse com esse curso é que é uma área que hoje é como vocês sabem tatá explodindo muitas pessoas a acham que essa explosão do interesse e inteligência artificial às vezes é uma coisa criada pela mídia é porque porque porque hoje você abre qualquer jornal você assistir qualquer canal de tv você abre qualquer revista vai ter alguma coisa de inteligência artificial ea área que domina hoje a agência oficial que é machina online então muita gente pergunta assistindo ao hype criado pela mídia tá eu estou aqui primeiro lugar para admitir cá isso e dizer que não tá então inteligência artificial é uma realidade hoje ela está presente na nossa vida muito mais que as pessoas sabem que as pessoas pensam tá e porque e na verdade esse hype da mídia é uma consequência do nosso avanço ataque tenha ocorrido na área na área da ciência na área da da empresa então a mídia está começando a se dar conta dessas mudanças que estão acontecendo a nossa área e por que tá acontecendo essas mudanças nos últimos cinco dez anos são por três fatores principais em primeiro lugar à direita então a gente nunca teve tantos dados disponíveis para treinar os nossos modelos de machine aí a gente vai ver que esses modelos de fato necessita de muitos dados ao contrário da gente que aprende coisas novas já tem uma noção da realidade os modelos estão aprendendo 02 opção de muitos dados em segundo lugar capacidade computacional então finalmente os computadores novos modelos ainda leva muito tempo a rodar é um dos modelos mais complexos que nós fazemos no meu laboratório vou falar um pouco sobre sobre ele que o lobby da pepsi eles ainda levam bastante tempo mas cada vez menos tempo com os alunos computacionais e principalmente os avanços na área de gpu que tem permitido rodar modelos de plano em cada vez mais rápidos e terceiro lugar o desenvolvimento técnico da o aparecimento de novos algoritmos the machine muitos desses algoritmos the machine já existem há muito tempo desde a década de 60 70 e 80 mas alguns deles são relativamente novos e algumas pequenos ajustes têm ocorrido umas pequenas inovações de ajustes a de parafusos aqui e ali técnicas de otimização a para a paralisação a função de perda diferente então a gente está sendo alguns avanços técnicos e nos últimos anos principalmente nessa área está cada vez mais forte e mash lane que a gente vai falar no curso que é de plani então só para deixar claro esse crescimento da área hoje a demanda por profissionais de saúde profissionais que sabem analisar machine lane há muitas muitas vezes na área da saúde é gigantesca hoje tá então hoje as empresas estão de fato pagando salários milionários para especialistas em marketing online e é uma das áreas mais quentes do momento aí as empresas não estão conseguindo preencher essas vagas porque devido à falta a de profissionais da área é demanda por profissionais na área científica também está bastante alta como a gente vê todos os dias no meu laboratório então pesquisa do do linkedin o link faça pesquisa todo ano é das várias habilidades que você consegue colocar o currículo do linkedin com a habilidade que conseguem emprego mais rapidamente tá e todos os anos no brasil há já vários anos é a análise de dados então você ter habilidade e analisar dados é a tua habilidade que consegue um emprego mais rapidamente e o emprego de qualidade mais rapidamente porque eu quis deixar isso claro nessa demanda que já se demanda porque se você tiver trabalhando em uma empresa que não está de acordo com os padrões éticos você precisa sair dessa empresa a demanda profissionais de marketing online é muito alta hoje e você não precisa participar dessas dessas empresas então a gente vai usar uma finlândia para o bem vamos fazer muita coisa a positiva provas a humanidade como a finlândia mas algumas empresas vão usar uma senha única coisa errada a gente tem muita empresa hoje é que finge que está aqui objetivo é conectar o mundo não é na verdade está manipulando voto estão manipulando sentimento já existem hoje empresas que utilizam uma chicane para vigilância de funcionários em fábricas por exemplo a gente hoje empresas que estão usando uma china que alimenta a limitar o acesso a bens e serviços a pessoas com necessidades se você está trabalhando em alguma das empresas você precisa sair da empresa a demanda pelo seu trabalho é muito grande você vai conseguir emprego muito bom fora dessas áreas não contribua à para essas a pressa as empresas e por outro lado há muitas empresas numa chicane para o bem dá pra fazer grandes mudanças positivas na sociedade como por exemplo o google está usando machiline para evitar diminuir o desmatamento da amazônia sim existem a empresa várias pesquisas melhorando o trânsito nas cidades mas sim lane para melhorar o trânsito nas grandes cidades e desenvolvimento em tecnologias verdes nec nosso laboratório a gente usa para melhorar a situação de saúde do brasil é melhorar políticas públicas e melhorar o diagnóstico dos pacientes etc então tem muita coisa boa ter feito em washington de começar o curso que deixar isso claro para você se torna essa empresa não precisa dessa empresa a demanda por você é muito grande ver contribuir para para o bem da sociedade como shane que é uma que é uma técnica uma ferramenta muito poderosa para ser usado para coisas pra coisas erradas então na área acadêmica que tipo de na área de pesquisa científica que tipo de estudo estão aparecendo que já usam mexe lane por exemplo um estudo publicado há o site teve como objetivo para dizer a presença de transtorno de estresse pós traumático utilizando dados de 24 países incluindo o brasil totalizando cerca de 69 mil indivíduos a prevalência do transtorno é relativamente baixo à população cerca de 4% então qual foi o objetivo do estudo pra dizer baseado em várias variáveis por editoras como sócio demográficas distúrbios mentais tipo de evento traumático é certa quem provavelmente a a desenvolver transtorno de estresse pós traumático eles usaram o super lana que o algoritmo de the machine e o que encontraram que os 10% de pessoas que o algoritmo falava que tinha maior risco incluíam praticamente todas as pessoas 95 por cento e noventa e cinco a seis por cento das pessoas que de fato tinha dois na então imaginem uma aplicação prática no hospital já que tem um limite de atendimentos a priorizar alguns pacientes prioriza esses 10% apenas você vai ter praticamente todo mundo aqui vai ter esse esse transtorno os 10% que o algoritmo fala que tem maior risco incluem basicamente todo mundo que vai desenvolver esse transtorno outro artigo recente que sair esse ano a procurou para dizer quais mulheres grávidas tinham maior risco de ter um evento adverso no parto que seria esse evento adverso filho prematuro combate peso ou internação na uti ea morte no primeiro ano tá aí pra que isso para orientar a inscrição no bar bar tal tucanos que é o programa nos estados unidos de prevenção já que ajuda a gravidade a previne a o desenvolvimento desses desses problemas adversa nascimento eles testaram quatro algoritmos the machine lane para predizer risco de parto adverso e compararam com a regra anterior a regra anterior a gente vê muito isso serviço de saúde e às vezes tem umas regras para a tomada de são luís são muito simples e que a china está trazendo tá melhorando os critérios de inclusão nos programas então existe uma regra que 17 fatores de risco se tivesse 2 a pessoa incluída no programa daí que os algoritmos mostraram que eles têm muito melhor capacidade preditiva de quem vai ter esse evento adverso do que essa regra anterior tha e os autores chegaram à conclusão que a cada duas mil mulheres o algoritmo incluiria mais 170 mulheres que de fato eu te desfecho adverso no parto então você inclui muito mais as pessoas que de fato você ia conseguir prevenir esses problemas no parto eu falei dos artigos científicos agora falar um pouco sobre o nosso laboratório que a oab da pepsi laboratório de big data e análise preditiva em saúde da faculdade de saúde pública da usp apesar da enorme crise dada ciência que a gente vive hoje a gente tem o financiamento a gente a sorte o financiamento da fapesp cnpq da e da fundação lemann a e que tipo de perguntas ele está respondendo nosso tentando responder nosso laboratório está em primeiro lugar é possível predizer como boa a qualidade preditiva quem vai morrer em breve e por qual causa a pessoa vai morrer que é uma das grandes questões da epidemiologia desde os clássicos estudos de um grau de 1.662 que ele procurava ver quais fatores estão associados ao óbito nos próximos anos a probabilidade de óbito porque isso é importante pra gente conseguir determinar que essa pessoa tem alta probabilidade de ter um infarto por exemplo eu tenho um derrame por exemplo a gente consegue iniciar medidas preventivas a com bastante antecedência ea gente descobriu né as pesquisas recentes que isso é um problema resolvido então mesmo com poucos dados ea gente tem um tutorial a e publicação né o tutorial de uso de machine com foco em prevenção de óbitos em idosos que a gente consegue predizer relativamente bem mesmo com poucos dados que vai morrer nos próximos cinco anos é principalmente idosos ea gente conseguiu agora acesso a um banco de dados bastante grande da inglaterra que a gente está bastante otimista que a gente vai conseguir não só para dizer que vai morrer mas também por qual causa de obras que as pessoas vão morrer outro outro desafio que a gente está tentando abordar a predizer qualidade de vida futura em pacientes com doenças graves então a gente está analisando uma mostra uma pesquisa do hospital do coração hcor é que eles fizeram um estudo com pacientes com câncer internados não tem orientado para dizer esse paciente que são graves a paciente com câncer internados na uti é quanto tempo de qualidade de vida eles têm pela frente é porque isso é importante pra pensar em orientar cuidados paliativos então os pacientes têm poucos dias poucas semanas pouco apenas um mês por exemplo de qualidade de vida pela frente dá essa possibilidade do paciente e profissional de saúde tomar essa decisão você quer continuar hospital ou você quer passar os últimos dias tentando melhorar de minerador melhorar a qualidade de vida na sua casa com seus familiares seus amigos em terceiro lugar a outro outro grande desafio do nosso laboratório identificar boas práticas de políticas públicas em saúde a identificar quais municípios brasileiros estão tendo uma boa gestão e saúde o que é bastante difícil né então como é que realmente você vê é bastante na mídia nos jornais um ranking é normalmente de por exemplo expectativa de vida então fica implícito que o município com maior expectativa de vida tem a melhor de sua saúde e seu absurdo daí porque que os municípios mais ricos vão ter melhor expectativa de vida então isso é simplesmente um ranking de riqueza dos municípios no ranking de qualidade de gestão e saúde a gente tava na área da saúde a gente sabe muito bem que fatores socioeconômicos fundamentais pra ver a qualidade de saúde da população então o que a gente está fazendo com o conheço tudo no meu lugar a gente está tentando predizer na testado é possível predizer qual é a expectativa de vida dos municípios brasileiros sem usar informações saúde só usando informação sócio econômica e demográfica então por exemplo dada a renda desse município o desemprego desse município a aaa escolaridade esse município de certas você tem 60 variáveis do nosso algoritmo consegue falar o município com as características tem expectativa de vida de 73 5 a 2 ea gente olhar de fato e 73 dos ladrões eu vou mostrar pra vocês que a gente consegue tá e daí a gente entra no segundo desafio que é por mais um município a 7 mas tão anti ele erra alguns casos alguns municípios vão melhor do que deveriam ir do que a predição eu sou melhor do que eles deveriam ir que a gente chama de owach vers a gente está chamando esse estudo de ovar ativas a gente entra nessa segunda fase depois que identificar o que estou ativos diferem do sandero artigos municípios que vão pior do que o nosso algoritmo disse que ele deveria ir aí sim comparar a situação de saúde para ver boas práticas de políticas públicas tá então primeiro lugar a gente mostra que consegue predizer com boa performance a expectativa de vida dos municípios então aqui né no gráfico se conseguem ver que a medida que nosso algoritmo fala que até que a expectativa de vida deve ser alta a gente vai lá cheque de fato é alto o nosso objetivo fala que deve ser baixa a gente checa de fato é baixo quando ele comete em geral poucos erros mas mesmo assim tem alguns outline em alguns municípios que o nosso algoritmo é bom ou melhor que o esperado ou pior que o esperado ea gente entra na segunda fase que já adianto para vocês nosso artigo está em fase final de revisão em uma revista científica aí que a gente encontrou que houver tiver se investem mais em atenção primária à saúde então são os municípios que têm mais tato é só da família passa o município tem mais base na ação são municípios têm mais programas federais de saúde e odontológico por exemplo tá isso é o resultado que a gente esperava a gente ficou muito feliz no fim é porque isso confirmou que os outros estudos têm mostrado com uma técnica totalmente diferente do que tem sido feito até hoje e mais o mais interessante disso é que a gente viu que o sandro ativas ou seja os municípios que vão pior do que deveriam ir do que o nosso algoritmo prediz que ele iria também investe em saúde e são melhores em algumas áreas mas em atenção secundário tá então são municípios que têm mais máquinas de raio x do que os outros são municípios que fazem mais seis áreas do que os outros municípios que fazer mais uma monografia do que os outros já então interessante aqui de fato eles estão também investindo em saúde mas tem investido em áreas com o retorno marginal menor esses são alguns exemplos de pesquisas na área pesquisas de outros grupos e nosso grupo mas como além desse existem milhares de outros grupos do mundo né fazendo essas pesquisas agora trabalhando isso agora mandando os ativos agora então é uma área da saúde relativamente nova que está explodindo agora num no máximo no último ano que é quando a gente entrou também nessa área [Música]')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate.from_messages([\n",
        "    ('system,' 'Você é uma assistente amigável que possui as seguintes informações: {informacoes}'),\n",
        "    ('user','{input}')\n",
        "])"
      ],
      "metadata": {
        "id": "8DyaezNC6YS2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_youtube = template | chat\n",
        "resposta = chain_youtube.invoke({'informacoes': lista_documentos, 'input': 'Quem apresenta o conteúdo? consegue detalhar?'})\n",
        "print(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqmwI45y59vx",
        "outputId": "39eb6d28-349b-4f86-a716-114d38d11cf6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O conteúdo apresentado é por Alexandre, um professor da Faculdade de Saúde Pública da USP (Universidade de São Paulo). Ele é especializado em estatísticas de saúde e análise de dados, e é fascinado pela aplicação da inteligência artificial na área da saúde.\n",
            "\n",
            "Alexandre apresenta o conteúdo de um curso de inteligência artificial com foco na área da saúde, e começa explicando que a área de inteligência artificial está explodindo atualmente, com muitas empresas e indústrias investindo nessa tecnologia. Ele destaca que a demanda por profissionais com habilidades em análise de dados e inteligência artificial é muito alta, e que as empresas estão dispostas a pagar salários altos para especialistas nessa área.\n",
            "\n",
            "Ele também destaca que a inteligência artificial não é apenas uma ferramenta para fins comerciais, mas também pode ser usada para melhorar a saúde pública e a qualidade de vida das pessoas. Ele cita exemplos de pesquisas científicas que utilizam inteligência artificial para prever a probabilidade de óbito em pacientes com doenças graves, e para identificar boas práticas de políticas públicas em saúde.\n",
            "\n",
            "Além disso, Alexandre apresenta o trabalho de seu laboratório, o Laboratório de Big Data e Análise Preditiva em Saúde, da Faculdade de Saúde Pública da USP, que está trabalhando em projetos de pesquisa que utilizam inteligência artificial para predizer a qualidade de vida futura em pacientes com doenças graves, e para identificar boas práticas de políticas públicas em saúde.\n",
            "\n",
            "Em resumo, Alexandre é um professor e pesquisador que está trabalhando na interseção da saúde pública e da inteligência artificial, e está apresentando o conteúdo de um curso que visa ensinar aos estudantes como aplicar a inteligência artificial na área da saúde.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_youtube = template | chat\n",
        "resposta = chain_youtube.invoke({'informacoes': lista_documentos, 'input': 'Há quanto tempo Alexandre é professor?'})\n",
        "print(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjwb11jr-iJ7",
        "outputId": "fba8bde4-eae9-4f69-bb9f-d6c6304e4f5d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A transcrição não fornece informações específicas sobre o tempo de Alexandre como professor. No entanto, é possível inferir que ele é professor há algum tempo, pois ele menciona que é economista e que se tornou fascinado pela análise de dados, especialmente na área da saúde. Além disso, ele é apresentado como professor da Faculdade de Saúde Pública da USP, o que sugere que ele tenha experiência acadêmica.\n",
            "\n",
            "É importante notar que a transcrição não fornece informações específicas sobre a idade ou a experiência de Alexandre como professor. Portanto, não é possível determinar exatamente há quanto tempo ele é professor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_youtube = template | chat\n",
        "resposta = chain_youtube.invoke({'informacoes': lista_documentos, 'input': 'Qual a principal mensagem do vídeo?'})\n",
        "print(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YguHsfIP-U4K",
        "outputId": "1f5bb679-6d42-4ea4-94ce-64eff56f3102"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A principal mensagem do vídeo é que a inteligência artificial (IA) é uma ferramenta poderosa que pode ser usada para melhorar a saúde pública e a qualidade de vida das pessoas, mas também é importante lembrar que ela pode ser usada para fins positivos ou negativos.\n",
            "\n",
            "O palestrante, Alexandre, destaca que a IA é uma realidade atual e está sendo usada em diversas áreas, incluindo a saúde, e que é importante entender como ela funciona e como pode ser usada para melhorar a saúde pública.\n",
            "\n",
            "Ele também destaca que a IA pode ser usada para predizer doenças, melhorar a qualidade de vida dos pacientes e identificar boas práticas de políticas públicas em saúde.\n",
            "\n",
            "Além disso, Alexandre enfatiza a importância de usar a IA de forma ética e responsável, e de não contribuir para a manipulação ou exploração de pessoas ou dados.\n",
            "\n",
            "Em resumo, a principal mensagem do vídeo é que a IA é uma ferramenta poderosa que pode ser usada para melhorar a saúde pública e a qualidade de vida das pessoas, mas é importante usar essa ferramenta de forma ética e responsável.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_youtube = template | chat\n",
        "resposta = chain_youtube.invoke({'informacoes': lista_documentos, 'input': 'Caso a IA seja usada de forma não ética, o que pode acontecer e o que o professor sugere?'})\n",
        "print(resposta.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raDje3IK-fML",
        "outputId": "e6a4f590-8441-432b-8299-71bc6336be8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se a IA for usada de forma não ética, podem ocorrer consequências negativas, como:\n",
            "\n",
            "* Manipulação de opiniões e comportamentos\n",
            "* Vigilância e controle de indivíduos\n",
            "* Discriminação e exclusão de grupos específicos\n",
            "* Uso indevido de dados pessoais\n",
            "* Dano à privacidade e à segurança\n",
            "\n",
            "O professor sugere que, caso você esteja trabalhando em uma empresa que está usando a IA de forma não ética, você deve sair da empresa. Ele também destaca que a demanda por profissionais de IA é alta e que você pode encontrar um emprego em uma empresa que use a IA de forma ética e responsável.\n",
            "\n",
            "Além disso, o professor enfatiza a importância de usar a IA para o bem da sociedade, como por exemplo, para melhorar a saúde pública, reduzir o desmatamento da Amazônia, melhorar o trânsito nas cidades, etc.\n",
            "\n",
            "Em resumo, o professor sugere que devemos ser conscientes do uso ético da IA e tomar medidas para evitar seu uso indevido, além de buscar oportunidades para usar a IA para o bem da sociedade.\n"
          ]
        }
      ]
    }
  ]
}
